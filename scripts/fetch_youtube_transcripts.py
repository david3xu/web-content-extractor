#!/usr/bin/env python
"""Fetch YouTube transcripts for links stored in output/youtube_links.json.

This script reads the JSON file generated by the extraction pipeline that contains
unique YouTube (or proxied) URLs, attempts to resolve the underlying YouTube
video ID, retrieves the transcript, and writes it to a Markdown (.md) file.

Usage:
    python scripts/fetch_youtube_transcripts.py

Requirements:
    poetry add youtube-transcript-api
    (or) pip install youtube-transcript-api

The script is intentionally dependency-light and will skip videos without an
available transcript rather than failing.
"""
from __future__ import annotations

import asyncio
import json
import re
import sys
from pathlib import Path

try:
    from youtube_transcript_api import (
        NoTranscriptFound,
        TranscriptsDisabled,
        VideoUnavailable,
        YouTubeTranscriptApi,
    )
except ImportError as exc:  # pragma: no cover
    sys.stderr.write(
        "youtube-transcript-api is required. Install it with `poetry add youtube-transcript-api` or `pip install youtube-transcript-api`.\n"
    )
    raise exc

try:
    import httpx
except ImportError as exc:  # pragma: no cover
    sys.stderr.write(
        "httpx is required. Install it with `poetry add httpx` or `pip install httpx`.\n"
    )
    raise exc

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent  # project root
INPUT_JSON = BASE_DIR / "output" / "youtube_links.json"
OUTPUT_DIR = BASE_DIR / "output" / "transcripts"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------
VIDEO_ID_RE = re.compile(r"(?:v=|/)([0-9A-Za-z_-]{11})(?:[?&#/]|$)")
EMBED_RE = re.compile(r"/embed/([0-9A-Za-z_-]{11})")
SHORT_RE = re.compile(r"youtu\.be/([0-9A-Za-z_-]{11})")


def extract_video_id(url: str) -> str | None:
    """Attempt to extract a YouTube video ID from a standard YouTube URL."""
    for pattern in (VIDEO_ID_RE, EMBED_RE, SHORT_RE):
        match = pattern.search(url)
        if match:
            return match.group(1)
    return None


async def resolve_iframe_ly(url: str) -> str | None:
    """Resolve cdn.iframe.ly proxy links to obtain the underlying YouTube ID.

    For iframe.ly URLs we perform an HTTP GET (with redirects) and inspect the
    final URL. If the final URL points to YouTube, we extract the video ID. If
    not, we attempt to search the returned HTML for an embed URL containing the
    ID.
    """
    try:
        async with httpx.AsyncClient(follow_redirects=True, timeout=10) as client:
            response = await client.get(url)
            response.raise_for_status()

            # First attempt: final resolved URL
            final_url = str(response.url)
            vid = extract_video_id(final_url)
            if vid:
                return vid

            # Second attempt: search in HTML meta tags / JSON-LD
            candidate = EMBED_RE.search(response.text) or VIDEO_ID_RE.search(
                response.text
            )
            if candidate:
                return candidate.group(1)
    except Exception as exc:  # pragma: no cover
        sys.stderr.write(f"⚠️  Failed to resolve {url}: {exc}\n")
    return None


async def get_video_id(url: str) -> str | None:
    """Get YouTube video ID from a URL, handling iframe.ly proxies."""
    # Direct extraction first
    vid = extract_video_id(url)
    if vid:
        return vid

    # Handle iframe.ly proxied URLs
    if "iframe.ly" in url:
        return await resolve_iframe_ly(url)

    return None


async def fetch_transcript(video_id: str) -> str | None:
    """Fetch transcript for a YouTube video ID.

    Returns the transcript formatted as Markdown (bullet list with timestamps),
    or None if unavailable.
    """
    try:
        segments = YouTubeTranscriptApi.get_transcript(
            video_id, languages=["en", "en-US", "en-GB"]
        )
        lines = [f"- [{seg['start']:.2f}s] {seg['text']}" for seg in segments]
        return "\n".join(lines)
    except (
        TranscriptsDisabled,
        NoTranscriptFound,
        VideoUnavailable,
    ) as exc:  # pragma: no cover
        sys.stderr.write(f"⚠️  Transcript not available for {video_id}: {exc}\n")
    except Exception as exc:  # pragma: no cover
        sys.stderr.write(f"⚠️  Failed to fetch transcript for {video_id}: {exc}\n")
    return None


async def process_urls(urls: list[str]) -> None:
    """Main processing coroutine."""
    total = len(urls)
    for idx, url in enumerate(urls, 1):
        print(f"[{idx}/{total}] Processing {url}…")
        vid = await get_video_id(url)
        if not vid:
            print("   ↳ Could not determine video ID, skipping.")
            continue

        md_path = OUTPUT_DIR / f"{vid}.md"
        if md_path.exists():
            print("   ↳ Transcript already exists, skipping.")
            continue

        transcript_md = await fetch_transcript(vid)
        if transcript_md:
            md_path.write_text(transcript_md, encoding="utf-8")
            print(f"   ↳ Saved transcript to {md_path.relative_to(BASE_DIR)}")
        else:
            print("   ↳ No transcript available.")


def main() -> None:
    if not INPUT_JSON.exists():
        sys.stderr.write(
            f"Input file {INPUT_JSON} not found. Generate youtube_links.json first.\n"
        )
        sys.exit(1)

    with INPUT_JSON.open(encoding="utf-8") as f:
        # We expect a JSON array of URLs. Allow broader types and validate manually.
        loaded = json.load(f)

    if not isinstance(loaded, list):
        sys.stderr.write("youtube_links.json must contain a JSON array of URLs.\n")
        sys.exit(1)

    urls: list[str] = [str(u) for u in loaded]

    asyncio.run(process_urls(urls))


if __name__ == "__main__":
    main()
